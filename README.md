# mlx-langchain-lite

**mlx-langchain-lite** ist ein leichtgewichtiges, vollstÃ¤ndig lokales RAG-Modul fÃ¼r MLX-basierte KI-Systeme.  
Es ersetzt LangChain fÃ¼r spezifische AnwendungsfÃ¤lle in KMU-nahen KI-Backends wie "Theseus-TeamMind".

---

## ğŸ§­ Projektziel

Entwicklung eines modularen RAG-Systems mit folgenden Eigenschaften:

- ğŸ’» Lokal ausfÃ¼hrbar (kompatibel mit Apple Silicon und MLX)
- ğŸ§  Inferenz mit MLX-basierten Text-Encodern & LLMs
- ğŸ§© UnterstÃ¼tzung von Multi-User- und Agenten-Szenarien
- ğŸ“ Indexierung von PDFs oder Dokument-Chunks
- ğŸš€ BatchfÃ¤hige Anfrageverarbeitung (RAG mit mehreren Prompts)
- âŒ Kein Einsatz von LangChain oder Cloud-Diensten

---

## ğŸ—‚ï¸ Bisher enthaltene Module

| Datei                   | Funktion                                                |
|-------------------------|---------------------------------------------------------|
| `embedding_engine.py`   | Einbettung von Texten Ã¼ber MLX-kompatible Encoder       |
| `vector_store.py`       | Verwaltung und Abfrage von FAISS-basierten Indizes      |
| `rag_handler.py`        | RAG-Abfrage, Prompt-Zusammenbau                         |
| `batch_dispatcher.py`   | Batchverarbeitung paralleler Anfragen                   |
| `LICENSE`               | Apache 2.0 Lizenz                                       |
| `README.md`             | Diese Datei                                             |

---

## ğŸ“Œ Wiedereinstieg bei spÃ¤terer Session

Wenn du in einem neuen Chat-Fenster mit ChatGPT weitermachen willst:

1. Ã–ffne dieses Projektverzeichnis.
2. Lade alle `.py`-Dateien und diese `README.md` hoch.
3. Schreibe:  
   **"Lass uns mit `mlx-langchain-lite` weitermachen, hier sind die bisherigen Dateien und das README."**

Dann erkennt ChatGPT automatisch, worum es geht, und du kannst mit der Implementierung fortfahren.

---

## ğŸ“‹ NÃ¤chste geplante Schritte

- [ ] Implementierung von `embedding_engine.py` (MLX-Encoder laden & anwenden)
- [ ] Erstellung einer Nutzer-basierten Vektorstruktur in `vector_store.py`
- [ ] Integration eines konfigurierbaren FAISS-Index pro Nutzer / Agent
- [ ] Anbindung an `Theseus-TeamMind` API (`/chat/rag`, `/data/upload/pdf`)
- [ ] Optional: PII-Filterung vorschalten (z.â€¯B. via Presidio)

---

## ğŸ§  Optional verwendbare MLX-Encoder

- `mlx-community/gte-small`
- `mlx-community/all-MiniLM-L6-v2`
- Lokale Konvertierung von GGUF â†’ MLX Ã¼ber eigenes Tooling

---

## ğŸ§‘â€ğŸ’» Projektstatus

**Stand:** Grundstruktur und Projektziel definiert  
**Bereit fÃ¼r:** erste Modellanbindung und Embedding-Test

---

## Lizenz

Dieses Projekt steht unter der [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0).

---
